{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMqJlrgJGCDUKuinj+VppJy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/datafilik/mobile-nextgen-wireless-comms-rsrch/blob/master/neuralMimoOfdmSystemV1_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_PbDlWZUzYl"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Created on Thu Oct 20 04:10:59 2022\n",
        "@Title: Mutliuser MIMO OFDM sytem with neural receiver simualtions\n",
        "@author: voche\n",
        "\"\"\"\n",
        "# Configure the notebook to use only a single GPU and allocate only as much memory as needed\n",
        "# For more details, see https://www.tensorflow.org/guide/gpu\n",
        "import tensorflow as tf\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "print('Number of GPUs available :', len(gpus))\n",
        "if gpus:\n",
        "    gpu_num = 0 # Index of the GPU to use\n",
        "    try:\n",
        "        tf.config.set_visible_devices(gpus[gpu_num], 'GPU')\n",
        "        print('Only GPU number', gpu_num, 'used.')\n",
        "        tf.config.experimental.set_memory_growth(gpus[gpu_num], True)\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "        \n",
        "#%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Layer, Conv2D, LayerNormalization\n",
        "from tensorflow.nn import relu\n",
        "\n",
        "# Import Sionna\n",
        "try:\n",
        "    import sionna\n",
        "except ImportError as e:\n",
        "    # Install Sionna if package is not already installed\n",
        "    import os\n",
        "    os.system(\"pip install sionna\")\n",
        "    import sionna\n",
        "\n",
        "from sionna.channel.tr38901 import Antenna, AntennaArray, CDL\n",
        "from sionna.channel import OFDMChannel\n",
        "from sionna.mimo import StreamManagement\n",
        "from sionna.ofdm import ResourceGrid, ResourceGridMapper, LSChannelEstimator, LMMSEEqualizer, RemoveNulledSubcarriers, ResourceGridDemapper\n",
        "from sionna.utils import BinarySource, ebnodb2no, insert_dims, flatten_last_dims, log10, expand_to_rank\n",
        "from sionna.fec.ldpc.encoding import LDPC5GEncoder\n",
        "from sionna.fec.ldpc.decoding import LDPC5GDecoder\n",
        "from sionna.mapping import Mapper, Demapper\n",
        "from sionna.utils.metrics import compute_ber\n",
        "from sionna.utils import sim_ber\n",
        "\n",
        "############################################\n",
        "## Channel configuration\n",
        "carrier_frequency = 28e9 # Hz\n",
        "delay_spread = 100e-9 # s\n",
        "cdl_model = \"C\" # CDL model to use\n",
        "#speed = 10.0 # Speed for evaluation and training [m/s]\n",
        "# SNR range for evaluation and training [dB]\n",
        "ebno_db_min = -5.0\n",
        "ebno_db_max = 10.0\n",
        "\n",
        "############################################\n",
        "## OFDM waveform configuration\n",
        "subcarrier_spacing = 30e3 # Hz\n",
        "fft_size = 128 # Number of subcarriers forming the resource grid, including the null-subcarrier and the guard bands\n",
        "num_ofdm_symbols = 14 # Number of OFDM symbols forming the resource grid\n",
        "dc_null = True # Null the DC subcarrier\n",
        "num_guard_carriers = [5, 6] # Number of guard carriers on each side\n",
        "pilot_pattern = \"kronecker\" # Pilot pattern\n",
        "pilot_ofdm_symbol_indices = [2, 11] # Index of OFDM symbols carrying pilots\n",
        "cyclic_prefix_length = 0 # Simulation in frequency domain. This is useless\n",
        "\n",
        "############################################\n",
        "## Modulation and coding configuration\n",
        "num_bits_per_symbol = 2 # QPSK\n",
        "coderate = 0.5 # Coderate for LDPC code\n",
        "\n",
        "############################################\n",
        "## Neural receiver configuration\n",
        "num_conv_channels = 128 # Number of convolutional channels for the convolutional layers forming the neural receiver\n",
        "\n",
        "############################################\n",
        "## Training configuration\n",
        "num_training_iterations = 200 #30000 # Number of training iterations\n",
        "training_batch_size = 128 # Training batch size\n",
        "model_weights_path = \"neural_receiver_weights\" # Location to save the neural receiver weights once training is done\n",
        "\n",
        "############################################\n",
        "## Evaluation configuration\n",
        "results_filename = \"neural_receiver_results\" # Location to save the results\n",
        "\n",
        "stream_manager = StreamManagement(np.array([[1]]),1) # Receiver-transmitter association matrix. One stream per transmitter\n",
        "\n",
        "resource_grid = ResourceGrid(num_ofdm_symbols = num_ofdm_symbols,\n",
        "                             fft_size = fft_size,\n",
        "                             subcarrier_spacing = subcarrier_spacing,\n",
        "                             num_tx = 1,\n",
        "                             num_streams_per_tx = 1,\n",
        "                             cyclic_prefix_length = cyclic_prefix_length,\n",
        "                             dc_null = dc_null,\n",
        "                             pilot_pattern = pilot_pattern,\n",
        "                             pilot_ofdm_symbol_indices = pilot_ofdm_symbol_indices,\n",
        "                             num_guard_carriers = num_guard_carriers)\n",
        "\n",
        "# Codeword length. It is calculated from the total number of databits carried by the resource grid, and the number of bits transmitted per resource element\n",
        "n = int(resource_grid.num_data_symbols*num_bits_per_symbol)\n",
        "# Number of information bits per codeword\n",
        "k = int(n*coderate)\n",
        "\n",
        "ut_antenna = Antenna(polarization=\"single\",\n",
        "                     polarization_type=\"V\",\n",
        "                     antenna_pattern=\"38.901\",\n",
        "                     carrier_frequency=carrier_frequency)\n",
        "\n",
        "bs_array = AntennaArray(num_rows=1,\n",
        "                        num_cols=1,\n",
        "                        polarization=\"dual\",\n",
        "                        polarization_type=\"VH\",\n",
        "                        antenna_pattern=\"38.901\",\n",
        "                        carrier_frequency=carrier_frequency)\n",
        "\n",
        "\n",
        "class ResidualBlock(Layer):\n",
        "    r\"\"\"\n",
        "    This Keras layer implements a convolutional residual block made of two convolutional layers with ReLU activation, layer normalization, and a skip connection.\n",
        "    The number of convolutional channels of the input must match the number of kernel of the convolutional layers ``num_conv_channel`` for the skip connection to work.\n",
        "\n",
        "    Input\n",
        "    ------\n",
        "    : [batch size, num time samples, num subcarriers, num_conv_channel], tf.float\n",
        "        Input of the layer\n",
        "\n",
        "    Output\n",
        "    -------\n",
        "    : [batch size, num time samples, num subcarriers, num_conv_channel], tf.float\n",
        "        Output of the layer\n",
        "    \"\"\"\n",
        "\n",
        "    def build(self, input_shape):\n",
        "\n",
        "        # Layer normalization is done over the last three dimensions: time, frequency, conv 'channels'\n",
        "        self._layer_norm_1 = LayerNormalization(axis=(-1, -2, -3))\n",
        "        self._conv_1 = Conv2D(filters=num_conv_channels,\n",
        "                              kernel_size=[3,3],\n",
        "                              padding='same',\n",
        "                              activation=None)\n",
        "        # Layer normalization is done over the last three dimensions: time, frequency, conv 'channels'\n",
        "        self._layer_norm_2 = LayerNormalization(axis=(-1, -2, -3))\n",
        "        self._conv_2 = Conv2D(filters=num_conv_channels,\n",
        "                              kernel_size=[3,3],\n",
        "                              padding='same',\n",
        "                              activation=None)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z = self._layer_norm_1(inputs)\n",
        "        z = relu(z)\n",
        "        z = self._conv_1(z)\n",
        "        z = self._layer_norm_2(z)\n",
        "        z = relu(z)\n",
        "        z = self._conv_2(z) # [batch size, num time samples, num subcarriers, num_channels]\n",
        "        # Skip connection\n",
        "        z = z + inputs\n",
        "\n",
        "        return z\n",
        "\n",
        "class NeuralReceiver(Layer):\n",
        "    r\"\"\"\n",
        "    Keras layer implementing a residual convolutional neural receiver.\n",
        "\n",
        "    This neural receiver is fed with the post-DFT received samples, forming a resource grid of size num_of_symbols x fft_size, and computes LLRs on the transmitted coded bits.\n",
        "    These LLRs can then be fed to an outer decoder to reconstruct the information bits.\n",
        "\n",
        "    As the neural receiver is fed with the entire resource grid, including the guard bands and pilots, it also computes LLRs for these resource elements.\n",
        "    They must be discarded to only keep the LLRs corresponding to the data-carrying resource elements.\n",
        "\n",
        "    Input\n",
        "    ------\n",
        "    y : [batch size, num rx antenna, num ofdm symbols, num subcarriers], tf.complex\n",
        "        Received post-DFT samples.\n",
        "\n",
        "    no : [batch size], tf.float32\n",
        "        Noise variance. At training, a different noise variance value is sampled for each batch example.\n",
        "\n",
        "    Output\n",
        "    -------\n",
        "    : [batch size, num ofdm symbols, num subcarriers, num_bits_per_symbol]\n",
        "        LLRs on the transmitted bits.\n",
        "        LLRs computed for resource elements not carrying data (pilots, guard bands...) must be discarded.\n",
        "    \"\"\"\n",
        "\n",
        "    def build(self, input_shape):\n",
        "\n",
        "        # Input convolution\n",
        "        self._input_conv = Conv2D(filters=num_conv_channels,\n",
        "                                  kernel_size=[3,3],\n",
        "                                  padding='same',\n",
        "                                  activation=None)\n",
        "        # Residual blocks\n",
        "        self._res_block_1 = ResidualBlock()\n",
        "        self._res_block_2 = ResidualBlock()\n",
        "        self._res_block_3 = ResidualBlock()\n",
        "        self._res_block_4 = ResidualBlock()\n",
        "        # Output conv\n",
        "        self._output_conv = Conv2D(filters=num_bits_per_symbol,\n",
        "                                   kernel_size=[3,3],\n",
        "                                   padding='same',\n",
        "                                   activation=None)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        y, no = inputs\n",
        "\n",
        "        # Feeding the noise power in log10 scale helps with the performance\n",
        "        no = log10(no)\n",
        "\n",
        "        # Stacking the real and imaginary components of the different antennas along the 'channel' dimension\n",
        "        y = tf.transpose(y, [0, 2, 3, 1]) # Putting antenna dimension last\n",
        "        no = insert_dims(no, 3, 1)\n",
        "        no = tf.tile(no, [1, y.shape[1], y.shape[2], 1])\n",
        "        # z : [batch size, num ofdm symbols, num subcarriers, 2*num rx antenna + 1]\n",
        "        z = tf.concat([tf.math.real(y),\n",
        "                       tf.math.imag(y),\n",
        "                       no], axis=-1)\n",
        "        # Input conv\n",
        "        z = self._input_conv(z)\n",
        "        # Residual blocks\n",
        "        z = self._res_block_1(z)\n",
        "        z = self._res_block_2(z)\n",
        "        z = self._res_block_3(z)\n",
        "        z = self._res_block_4(z)\n",
        "        # Output conv\n",
        "        z = self._output_conv(z)\n",
        "\n",
        "        return z\n",
        "    \n",
        "class neuralEnabledMimoOfdmE2ESystem(Model):\n",
        "    r\"\"\"\n",
        "    Keras model that implements the end-to-end systems.\n",
        "\n",
        "    As the three considered end-to-end systems (perfect CSI baseline, LS estimation baseline, and neural receiver) share most of\n",
        "    the link components (transmitter, channel model, outer code...), they are implemented using the same Keras model.\n",
        "\n",
        "    When instantiating the Keras model, the parameter ``system`` is used to specify the system to setup,\n",
        "    and the parameter ``training`` is used to specified if the system is instantiated to be trained or to be evaluated.\n",
        "    The ``training`` parameter is only relevant when the neural\n",
        "\n",
        "    At each call of this model:\n",
        "    * A batch of codewords is randomly sampled, modulated, and mapped to resource grids to form the channel inputs\n",
        "    * A batch of channel realizations is randomly sampled and applied to the channel inputs\n",
        "    * The receiver is executed on the post-DFT received samples to compute LLRs on the coded bits.\n",
        "      Which receiver is executed (baseline with perfect CSI knowledge, baseline with LS estimation, or neural receiver) depends\n",
        "      on the specified ``system`` parameter.\n",
        "    * If not training, the outer decoder is applied to reconstruct the information bits\n",
        "    * If training, the BMD rate is estimated over the batch from the LLRs and the transmitted bits\n",
        "\n",
        "    Parameters\n",
        "    -----------\n",
        "    system : str\n",
        "        Specify the receiver to use. Should be one of 'baseline-perfect-csi', 'baseline-ls-estimation' or 'neural-receiver'\n",
        "\n",
        "    training : bool\n",
        "        Set to `True` if the system is instantiated to be trained. Set to `False` otherwise. Defaults to `False`.\n",
        "        If the system is instantiated to be trained, the outer encoder and decoder are not instantiated as they are not required for training.\n",
        "        This significantly reduces the computational complexity of training.\n",
        "        If training, the bit-metric decoding (BMD) rate is computed from the transmitted bits and the LLRs. The BMD rate is known to be\n",
        "        an achievable information rate for BICM systems, and therefore training of the neural receiver aims at maximizing this rate.\n",
        "\n",
        "    Input\n",
        "    ------\n",
        "    batch_size : int\n",
        "        Batch size\n",
        "\n",
        "    no : scalar or [batch_size], tf.float\n",
        "        Noise variance.\n",
        "        At training, a different noise variance should be sampled for each batch example.\n",
        "\n",
        "    Output\n",
        "    -------\n",
        "    If ``training`` is set to `True`, then the output is a single scalar, which is an estimation of the BMD rate computed over the batch. It\n",
        "    should be used as objective for training.\n",
        "    If ``training`` is set to `False`, the transmitted information bits and their reconstruction on the receiver side are returned to\n",
        "    compute the block/bit error rate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, system, speed, training=False):\n",
        "        super().__init__()\n",
        "        self._system = system\n",
        "        self._training = training\n",
        "\n",
        "        ######################################\n",
        "        ## Transmitter\n",
        "        self._binary_source = BinarySource()\n",
        "        # To reduce the computational complexity of training, the outer code is not used when training,\n",
        "        # as it is not required\n",
        "        if not training:\n",
        "            self._encoder = LDPC5GEncoder(k, n)\n",
        "        self._mapper = Mapper(\"qam\", num_bits_per_symbol)\n",
        "        self._rg_mapper = ResourceGridMapper(resource_grid)\n",
        "\n",
        "        ######################################\n",
        "        ## Channel\n",
        "        # A 3GPP CDL channel model is used\n",
        "        cdl = CDL(cdl_model, delay_spread, carrier_frequency,\n",
        "                  ut_antenna, bs_array, \"uplink\", min_speed=speed)\n",
        "        self._channel = OFDMChannel(cdl, resource_grid, normalize_channel=True, return_channel=True)\n",
        "\n",
        "        ######################################\n",
        "        ## Receiver\n",
        "        # Three options for the receiver depending on the value of `system`\n",
        "        if \"baseline\" in system:\n",
        "            if system == 'baseline-perfect-csi': # Perfect CSI\n",
        "                self._removed_null_subc = RemoveNulledSubcarriers(resource_grid)\n",
        "            elif system == 'baseline-ls-estimation': # LS estimation\n",
        "                self._ls_est = LSChannelEstimator(resource_grid, interpolation_type=\"nn\")\n",
        "            # Components required by both baselines\n",
        "            self._lmmse_equ = LMMSEEqualizer(resource_grid, stream_manager, )\n",
        "            self._demapper = Demapper(\"app\", \"qam\", num_bits_per_symbol)\n",
        "        elif system == \"neural-receiver\": # Neural receiver\n",
        "            self._neural_receiver = NeuralReceiver()\n",
        "            self._rg_demapper = ResourceGridDemapper(resource_grid, stream_manager) # Used to extract data-carrying resource elements\n",
        "        # To reduce the computational complexity of training, the outer code is not used when training,\n",
        "        # as it is not required\n",
        "        if not training:\n",
        "            self._decoder = LDPC5GDecoder(self._encoder, hard_out=True)\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, batch_size, ebno_db):\n",
        "\n",
        "        # If `ebno_db` is a scalar, a tensor with shape [batch size] is created as it is what is expected by some layers\n",
        "        if len(ebno_db.shape) == 0:\n",
        "            ebno_db = tf.fill([batch_size], ebno_db)\n",
        "\n",
        "        ######################################\n",
        "        ## Transmitter\n",
        "        no = ebnodb2no(ebno_db, num_bits_per_symbol, coderate)\n",
        "        # Outer coding is only performed if not training\n",
        "        if self._training:\n",
        "            c = self._binary_source([batch_size, 1, 1, n])\n",
        "        else:\n",
        "            b = self._binary_source([batch_size, 1, 1, k])\n",
        "            c = self._encoder(b)\n",
        "        # Modulation\n",
        "        x = self._mapper(c)\n",
        "        x_rg = self._rg_mapper(x)\n",
        "\n",
        "        ######################################\n",
        "        ## Channel\n",
        "        # A batch of new channel realizations is sampled and applied at every inference\n",
        "        no_ = expand_to_rank(no, tf.rank(x_rg))\n",
        "        y,h = self._channel([x_rg, no_])\n",
        "\n",
        "        ######################################\n",
        "        ## Receiver\n",
        "        # Three options for the receiver depending on the value of ``system``\n",
        "        if \"baseline\" in self._system:\n",
        "            if self._system == 'baseline-perfect-csi':\n",
        "                h_hat = self._removed_null_subc(h) # Extract non-null subcarriers\n",
        "                err_var = 0.0 # No channel estimation error when perfect CSI knowledge is assumed\n",
        "            elif self._system == 'baseline-ls-estimation':\n",
        "                h_hat, err_var = self._ls_est([y, no]) # LS channel estimation with nearest-neighbor\n",
        "            x_hat, no_eff = self._lmmse_equ([y, h_hat, err_var, no]) # LMMSE equalization\n",
        "            no_eff_= expand_to_rank(no_eff, tf.rank(x_hat))\n",
        "            llr = self._demapper([x_hat, no_eff_]) # Demapping\n",
        "        elif self._system == \"neural-receiver\":\n",
        "            # The neural receover computes LLRs from the frequency domain received symbols and N0\n",
        "            y = tf.squeeze(y, axis=1)\n",
        "            llr = self._neural_receiver([y, no])\n",
        "            llr = insert_dims(llr, 2, 1) # Reshape the input to fit what the resource grid demapper is expected\n",
        "            llr = self._rg_demapper(llr) # Extract data-carrying resource elements. The other LLrs are discarded\n",
        "            llr = tf.reshape(llr, [batch_size, 1, 1, n]) # Reshape the LLRs to fit what the outer decoder is expected\n",
        "\n",
        "        # Outer coding is not needed if the information rate is returned\n",
        "        if self._training:\n",
        "            # Compute and return BMD rate (in bit), which is known to be an achievable\n",
        "            # information rate for BICM systems.\n",
        "            # Training aims at maximizing the BMD rate\n",
        "            bce = tf.nn.sigmoid_cross_entropy_with_logits(c, llr)\n",
        "            bce = tf.reduce_mean(bce)\n",
        "            rate = tf.constant(1.0, tf.float32) - bce/tf.math.log(2.)\n",
        "            return rate\n",
        "        else:\n",
        "            # Outer decoding\n",
        "            b_hat = self._decoder(llr)\n",
        "            return b,b_hat # Ground truth and reconstructed information bits returned for BER/BLER computation\n",
        "        \n",
        "# Range of SNRs over which the systems are evaluated\n",
        "ebno_dbs = np.arange(ebno_db_min, # Min SNR for evaluation\n",
        "                     ebno_db_max, # Max SNR for evaluation\n",
        "                     0.5) # Step"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STUDIES    \n",
        "# mobility studies\n",
        "# Dictionnary storing the evaluation results\n",
        "#BLER = {}\n",
        "\n",
        "BLER = {\n",
        "    \"baseline-perfect-csi\": [],\n",
        "    \"baseline-ls-estimation\": [],\n",
        "    \"neural-receiver\": []        \n",
        "}\n",
        "\n",
        "\n",
        "MOBILITY_SIMS = {\n",
        "    \"speed\" : [10] #[0.0, 20.0, 30.0]\n",
        "}\n",
        "\n",
        "for speed in MOBILITY_SIMS[\"speed\"]:\n",
        "\n",
        "    # Train neural receiver\n",
        "    model = neuralEnabledMimoOfdmE2ESystem('neural-receiver', speed=speed, training=True)\n",
        "    \n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "    \n",
        "    for i in range(num_training_iterations):\n",
        "        # Sampling a batch of SNRs\n",
        "        ebno_db = tf.random.uniform(shape=[], minval=ebno_db_min, maxval=ebno_db_max)\n",
        "        # Forward pass\n",
        "        with tf.GradientTape() as tape:\n",
        "            rate = model(training_batch_size, ebno_db)\n",
        "            # Tensorflow optimizers only know how to minimize loss function.\n",
        "            # Therefore, a loss function is defined as the additive inverse of the BMD rate\n",
        "            loss = -rate\n",
        "        # Computing and applying gradients\n",
        "        weights = model.trainable_weights\n",
        "        grads = tape.gradient(loss, weights)\n",
        "        optimizer.apply_gradients(zip(grads, weights))\n",
        "        # Periodically printing the progress\n",
        "        if i % 100 == 0:\n",
        "            print('Iteration {}/{}  Rate: {:.4f} bit'.format(i, num_training_iterations, rate.numpy()), end='\\r')\n",
        "    \n",
        "    # Save the weights in a file\n",
        "    weights = model.get_weights()\n",
        "    with open(model_weights_path, 'wb') as f:\n",
        "        pickle.dump(weights, f)\n",
        "        \n",
        "    \n",
        "    \n",
        "    model = neuralEnabledMimoOfdmE2ESystem('baseline-perfect-csi')\n",
        "    _,bler = sim_ber(model, ebno_dbs, batch_size=128, num_target_block_errors=100, max_mc_iter=100)\n",
        "    BLER['baseline-perfect-csi'].append(list(bler.numpy()))\n",
        "    \n",
        "    model = neuralEnabledMimoOfdmE2ESystem('baseline-ls-estimation')\n",
        "    _,bler = sim_ber(model, ebno_dbs, batch_size=128, num_target_block_errors=100, max_mc_iter=100)\n",
        "    BLER['baseline-ls-estimation'].append(list(bler.numpy()))\n",
        "    \n",
        "    model = neuralEnabledMimoOfdmE2ESystem('neural-receiver',speed=speed)\n",
        "    \n",
        "    # Run one inference to build the layers and loading the weights\n",
        "    model(1, tf.constant(10.0, tf.float32))\n",
        "    with open(model_weights_path, 'rb') as f:\n",
        "        weights = pickle.load(f)\n",
        "    model.set_weights(weights)\n",
        "    \n",
        "    # Evaluations\n",
        "    _,bler = sim_ber(model, ebno_dbs, batch_size=128, num_target_block_errors=100, max_mc_iter=100)\n",
        "    BLER['neural-receiver'].append(list(bler.numpy()))\n",
        "\n",
        "\n",
        "plt.xlabel(r\"$E_b/N_0$ (dB)\")\n",
        "plt.ylabel(\"BLER\")\n",
        "plt.grid(which=\"both\")\n",
        "\n",
        "i=0\n",
        "legend = []\n",
        "for speed in MOBILITY_SIMS[\"speed\"]:\n",
        "    # Baseline - Perfect CSI\n",
        "    plt.semilogy(ebno_dbs, BLER['baseline-perfect-csi'][i], label=\"Baseline - Perfect CSI {}[m/s]\".format(speed))\n",
        "    # Baseline - LS Estimation\n",
        "    plt.semilogy(ebno_dbs, BLER['baseline-ls-estimation'][i], label=\"Baseline - LS Estimation {}[m/s]\".format(speed))\n",
        "    # Neural receiver\n",
        "    plt.semilogy(ebno_dbs, BLER['neural-receiver'][i], label=\"Neural receiver {}[m/s]\".format(speed))\n",
        "    \n",
        "    i+= 1\n",
        "    \n",
        "plt.legend()\n",
        "plt.ylim((1e-4, 1.0))\n",
        "plt.tight_layout()\n",
        "#plt.title(\"Different 3GPP  CDL-{} Models Uplink - impact of UT mobility\".format(cdl_model));\n",
        "plt.title(\"Different 3GPP  CDL-{} Models Uplink - impact of Learning-based channel estimation\".format(cdl_model));"
      ],
      "metadata": {
        "id": "Jv7Z6YdwVR4U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}